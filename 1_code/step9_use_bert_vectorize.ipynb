{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_trf_bertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
    "#doc = nlp(\"Apple shares rose on the news. Apple pie is delicious.\")\n",
    "#print(doc[0].similarity(doc[7]))\n",
    "#print(doc._.trf_last_hidden_state.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# an example of spacy preserved some word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9134093016230975\n",
      "0.9053232267859165\n",
      "0.9737446020998256\n"
     ]
    }
   ],
   "source": [
    "doc1 = nlp(u'they did an awesome job')\n",
    "doc2 = nlp(u'they did an awful job')\n",
    "doc3 = nlp(u'they did a great job')\n",
    " \n",
    "print (doc1.similarity(doc2)) \n",
    "print (doc2.similarity(doc3)) \n",
    "print (doc1.similarity(doc3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(768,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2.vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0865907073020935"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "cosine(doc1.vector,doc2.vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the texts : step 1\n",
    "## Steps \n",
    "1. vectorize every piece of reviews(3k docs) in dictionary doc_vecs , key is review_id, value is the vector \n",
    "2. compute similarity and store them (3k * 3k) in distance_matrix \n",
    "3. For every month, mkt, find out how many reviews are out-there and retreive the pairwise distances. So if there are 10 reviews, need to retrieve 90 values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input: raw reviews texts from /0_data/Lock_ES_RawData/installer_review_data_20180410.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "reviews=pd.read_csv('../0_data/Lock_ES_RawData/installer_review_data_20180410.csv',sep=',',escapechar='\\\\')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we vectorized 0 reviews and time has elapsed 0.3196842670440674\n",
      "we vectorized 100 reviews and time has elapsed 60.20851683616638\n",
      "we vectorized 200 reviews and time has elapsed 100.01848268508911\n",
      "we vectorized 300 reviews and time has elapsed 139.13074707984924\n",
      "we vectorized 400 reviews and time has elapsed 174.80279302597046\n",
      "we vectorized 500 reviews and time has elapsed 212.08138346672058\n",
      "we vectorized 600 reviews and time has elapsed 252.14534330368042\n",
      "we vectorized 700 reviews and time has elapsed 288.0042989253998\n",
      "we vectorized 800 reviews and time has elapsed 326.5471487045288\n",
      "we vectorized 900 reviews and time has elapsed 371.2351825237274\n",
      "we vectorized 1000 reviews and time has elapsed 406.4341835975647\n",
      "we vectorized 1100 reviews and time has elapsed 449.0208420753479\n",
      "we vectorized 1200 reviews and time has elapsed 491.01585602760315\n",
      "we vectorized 1300 reviews and time has elapsed 520.635381937027\n",
      "we vectorized 1400 reviews and time has elapsed 562.695565700531\n",
      "we vectorized 1500 reviews and time has elapsed 610.7105603218079\n",
      "we vectorized 1600 reviews and time has elapsed 651.7199809551239\n",
      "we vectorized 1700 reviews and time has elapsed 703.5999507904053\n",
      "we vectorized 1800 reviews and time has elapsed 743.5231685638428\n",
      "we vectorized 1900 reviews and time has elapsed 797.0906529426575\n",
      "we vectorized 2000 reviews and time has elapsed 848.148188829422\n",
      "we vectorized 2100 reviews and time has elapsed 900.9291112422943\n",
      "we vectorized 2200 reviews and time has elapsed 950.3595876693726\n",
      "we vectorized 2300 reviews and time has elapsed 998.993533372879\n",
      "we vectorized 2400 reviews and time has elapsed 1084.9160747528076\n",
      "we vectorized 2500 reviews and time has elapsed 1139.6428236961365\n",
      "we vectorized 2600 reviews and time has elapsed 1207.1243810653687\n",
      "we vectorized 2700 reviews and time has elapsed 1262.167965888977\n",
      "we vectorized 2800 reviews and time has elapsed 1314.574630498886\n",
      "we vectorized 2900 reviews and time has elapsed 1355.630249261856\n",
      "we vectorized 3000 reviews and time has elapsed 1394.0766026973724\n",
      "we vectorized 3100 reviews and time has elapsed 1436.4688436985016\n",
      "we vectorized 3200 reviews and time has elapsed 1472.4966173171997\n",
      "we vectorized 3300 reviews and time has elapsed 1505.1921701431274\n",
      "we have processed 100000 pairs and time has elapsed 128.5166471004486\n",
      "we have processed 200000 pairs and time has elapsed 264.1646659374237\n",
      "we have processed 300000 pairs and time has elapsed 399.822336435318\n",
      "we have processed 400000 pairs and time has elapsed 534.9043056964874\n",
      "we have processed 500000 pairs and time has elapsed 671.0333013534546\n",
      "we have processed 600000 pairs and time has elapsed 799.2682926654816\n",
      "we have processed 700000 pairs and time has elapsed 950.6990842819214\n",
      "we have processed 800000 pairs and time has elapsed 1088.0720314979553\n",
      "we have processed 900000 pairs and time has elapsed 1226.527066707611\n",
      "we have processed 1000000 pairs and time has elapsed 1351.743581533432\n",
      "we have processed 1100000 pairs and time has elapsed 1479.1813633441925\n",
      "we have processed 1200000 pairs and time has elapsed 1617.574274301529\n",
      "we have processed 1300000 pairs and time has elapsed 1755.6262123584747\n",
      "we have processed 1400000 pairs and time has elapsed 1882.7890241146088\n",
      "we have processed 1500000 pairs and time has elapsed 1996.4601995944977\n",
      "we have processed 1600000 pairs and time has elapsed 2106.028268814087\n",
      "we have processed 1700000 pairs and time has elapsed 2214.198412179947\n",
      "we have processed 1800000 pairs and time has elapsed 2338.0614247322083\n",
      "we have processed 1900000 pairs and time has elapsed 2453.499540090561\n",
      "we have processed 2000000 pairs and time has elapsed 2566.8718090057373\n",
      "we have processed 2100000 pairs and time has elapsed 2680.850391149521\n",
      "we have processed 2200000 pairs and time has elapsed 2800.0731987953186\n",
      "we have processed 2300000 pairs and time has elapsed 2948.1223604679108\n",
      "we have processed 2400000 pairs and time has elapsed 3073.00199842453\n",
      "we have processed 2500000 pairs and time has elapsed 3214.871016740799\n",
      "we have processed 2600000 pairs and time has elapsed 3345.0101346969604\n",
      "we have processed 2700000 pairs and time has elapsed 3477.290759086609\n",
      "we have processed 2800000 pairs and time has elapsed 3599.211115837097\n",
      "we have processed 2900000 pairs and time has elapsed 3726.517145872116\n",
      "we have processed 3000000 pairs and time has elapsed 3866.486218214035\n",
      "we have processed 3100000 pairs and time has elapsed 3997.3989322185516\n",
      "we have processed 3200000 pairs and time has elapsed 4115.474193811417\n",
      "we have processed 3300000 pairs and time has elapsed 4230.705540418625\n",
      "we have processed 3400000 pairs and time has elapsed 4345.072148323059\n",
      "we have processed 3500000 pairs and time has elapsed 4459.558792591095\n",
      "we have processed 3600000 pairs and time has elapsed 4570.556710720062\n",
      "we have processed 3700000 pairs and time has elapsed 4683.662346363068\n",
      "we have processed 3800000 pairs and time has elapsed 4796.962800979614\n",
      "we have processed 3900000 pairs and time has elapsed 4908.382493257523\n",
      "we have processed 4000000 pairs and time has elapsed 5019.387346506119\n",
      "we have processed 4100000 pairs and time has elapsed 5131.125770568848\n",
      "we have processed 4200000 pairs and time has elapsed 5242.759595632553\n",
      "we have processed 4300000 pairs and time has elapsed 5353.837309122086\n",
      "we have processed 4400000 pairs and time has elapsed 5464.270047426224\n",
      "we have processed 4500000 pairs and time has elapsed 5574.789110660553\n",
      "we have processed 4600000 pairs and time has elapsed 5684.572751760483\n",
      "we have processed 4700000 pairs and time has elapsed 5798.931449651718\n",
      "we have processed 4800000 pairs and time has elapsed 5910.97345662117\n",
      "we have processed 4900000 pairs and time has elapsed 6021.540497303009\n",
      "we have processed 5000000 pairs and time has elapsed 6133.063902139664\n",
      "we have processed 5100000 pairs and time has elapsed 6245.755713701248\n",
      "we have processed 5200000 pairs and time has elapsed 6356.037822961807\n",
      "we have processed 5300000 pairs and time has elapsed 6468.6048538684845\n",
      "we have processed 5400000 pairs and time has elapsed 6579.6632561683655\n",
      "we have processed 5500000 pairs and time has elapsed 6692.316691875458\n",
      "we have processed 5600000 pairs and time has elapsed 6803.518679141998\n",
      "we have processed 5700000 pairs and time has elapsed 6914.773095607758\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "doc_vecs = []\n",
    "reviews=reviews[['id','body']].dropna()\n",
    "start_time = time.time()\n",
    "for i, row in reviews.iterrows():\n",
    "    doc_vecs.append([row.id,nlp(row['body']).vector])\n",
    "    if i%100==0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('we vectorized {} reviews and time has elapsed {}'.format(i,elapsed_time))\n",
    "\n",
    "import pandas as pd \n",
    "reviews_doc_vecs_df=pd.DataFrame(data=doc_vecs,columns=['reviewid','vectorized_text'])\n",
    "temp_df_doc_vecs=reviews_doc_vecs_df\n",
    "\n",
    "from scipy.spatial.distance import cosine\n",
    "from itertools import combinations \n",
    "\n",
    "start_time = time.time()\n",
    "distances_list=[]\n",
    "counter=0\n",
    "for item in combinations(temp_df_doc_vecs.reviewid,2):\n",
    "    #print(item)\n",
    "    #print(cosine(temp_df_doc_vecs.iloc[item[0]-1].vectorized_text,temp_df_doc_vecs.iloc[item[1]-1].vectorized_text))\n",
    "    vec1=temp_df_doc_vecs[temp_df_doc_vecs['reviewid']==item[0]].vectorized_text.tolist();\n",
    "    vec2=temp_df_doc_vecs[temp_df_doc_vecs['reviewid']==item[1]].vectorized_text.tolist();\n",
    "    d=cosine(vec1,vec2)\n",
    "    data_collect=[item[0],item[1],d]\n",
    "    distances_list.append(data_collect)  \n",
    "    counter=counter+1      \n",
    "    if counter%100000==0:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print('we have processed {} pairs and time has elapsed {}'.format(counter,elapsed_time))\n",
    "        distances_df=pd.DataFrame(data=distances_list,columns=['vec_review_id_1','vec_review_id_2','cosine_distance'])\n",
    "        filename='BERT_distances_pairwire_dec30'+str(counter/100000)+'.csv'\n",
    "        distances_df.to_csv(filename)\n",
    "        distances_list=[]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "distances_pairs=pd.DataFrame()\n",
    "for f in glob.glob('BERT_distances_pairwire_dec30*.csv'):\n",
    "    df = pd.read_csv(f)\n",
    "    distances_pairs=pd.concat([df,distances_pairs])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: pairwise distances of all pairs  '../3_output/ALL_BERT_distances_pairwise_dec30.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances_pairs.to_csv('../3_output/ALL_BERT_distances_pairwise_dec30.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
